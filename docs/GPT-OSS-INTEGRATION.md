# GPT-OSS-120B Integration Guide

## Overview

GPT-OSS-120B is a powerful open-source large language model that provides local AI capabilities for Synapse AI. This integration allows you to run advanced medical AI analysis entirely on your own hardware, providing privacy, control, and reduced dependency on cloud services.

## üöÄ Quick Start

### 1. Installation

**Windows (PowerShell):**
```powershell
npm run install:gpt-oss
```

**Linux/macOS (Bash):**
```bash
chmod +x scripts/install-gpt-oss.sh
./scripts/install-gpt-oss.sh
```

### 2. Configuration

Add to your `.env` file:
```env
GPT_OSS_MODEL_PATH=./gpt-oss-120b/original
GPT_OSS_GPU_LAYERS=50
GPT_OSS_THREADS=8
GPT_OSS_CONTEXT_LENGTH=8192
GPT_OSS_MAX_TOKENS=2048
GPT_OSS_TEMPERATURE=0.1
```

### 3. Testing

```bash
npm run test:gpt-oss
```

## üìã System Requirements

### Minimum Requirements
- **RAM**: 32GB system RAM
- **Storage**: 250GB free space
- **CPU**: 8+ cores recommended
- **GPU**: Optional but highly recommended

### Recommended Requirements
- **RAM**: 64GB+ system RAM
- **Storage**: 500GB+ NVMe SSD
- **CPU**: 16+ cores (Intel i9/AMD Ryzen 9)
- **GPU**: RTX 4090 (24GB VRAM) or RTX 3090 (24GB VRAM)

### GPU Configuration Guide

| GPU Model | VRAM | Recommended GPU Layers | Performance |
|-----------|------|----------------------|-------------|
| RTX 4090 | 24GB | 50-60 | Excellent |
| RTX 3090 | 24GB | 40-50 | Very Good |
| RTX 3080 | 10GB | 20-30 | Good |
| RTX 3070 | 8GB | 15-25 | Fair |
| CPU Only | N/A | 0 | Slow |

## üîß Usage

### Basic Text Generation

```typescript
import { GPTOSSManager } from '@/services/gpt-oss-integration';

const gptOss = GPTOSSManager.getInstance();
await gptOss.initialize();

const response = await gptOss.generate(
  "Analyze a chest X-ray showing possible pneumothorax.",
  {
    maxTokens: 1024,
    temperature: 0.1,
    topP: 0.9
  }
);

console.log(response.text);
```

### Medical Diagnosis Flow

```typescript
import { gptOssDiagnosis } from '@/ai/flows/gpt-oss-diagnosis';

const diagnosis = await gptOssDiagnosis({
  radiologyMediaDataUris: [imageDataUri],
  mediaType: 'image',
  isDicom: false
});

console.log('Primary diagnosis:', diagnosis.primarySuggestion);
console.log('Confidence:', diagnosis.confidence);
console.log('Differentials:', diagnosis.differentialDiagnoses);
```

### REST API Usage

```bash
# Generate text
curl -X POST http://localhost:3000/api/gpt-oss \
  -H "Content-Type: application/json" \
  -d '{
    "action": "generate",
    "prompt": "What is pneumothorax?",
    "options": {
      "maxTokens": 500,
      "temperature": 0.1
    }
  }'

# Medical diagnosis
curl -X POST http://localhost:3000/api/gpt-oss \
  -H "Content-Type: application/json" \
  -d '{
    "action": "diagnose",
    "radiologyMediaDataUris": ["data:image/jpeg;base64,..."],
    "mediaType": "image"
  }'

# Check status
curl http://localhost:3000/api/gpt-oss
```

## üéØ Features

### Core Capabilities
- **Local Processing**: Complete privacy and control
- **Medical Optimization**: Specialized prompts for medical analysis
- **Streaming Support**: Real-time response generation
- **Performance Monitoring**: Built-in metrics and optimization
- **Error Handling**: Robust error recovery and reporting

### Medical AI Features
- **Diagnostic Analysis**: Comprehensive medical image analysis
- **Differential Diagnosis**: Multiple diagnostic possibilities with probabilities
- **Clinical Correlation**: Urgency assessment and follow-up recommendations
- **Quality Assessment**: Image quality and diagnostic confidence scoring
- **Knowledge Integration**: Automatic medical knowledge base lookups

### Integration Benefits
- **Hybrid Architecture**: Works alongside Google Gemini models
- **Fallback Support**: Automatic fallback to cloud models if needed
- **Performance Comparison**: Built-in benchmarking capabilities
- **Cost Efficiency**: No per-token costs after initial setup

## ‚öôÔ∏è Configuration Options

### Model Parameters

```typescript
interface GPTOSSConfig {
  modelPath: string;           // Path to model files
  maxTokens: number;           // Maximum response length (default: 2048)
  temperature: number;         // Creativity level (0.0-1.0, default: 0.1)
  topP: number;               // Nucleus sampling (default: 0.9)
  topK: number;               // Top-K sampling (default: 40)
  repetitionPenalty: number;   // Repetition penalty (default: 1.1)
  contextLength: number;       // Context window size (default: 8192)
  batchSize: number;          // Batch size (default: 1)
  gpuLayers: number;          // GPU layers (default: 50)
  threads: number;            // CPU threads (default: 8)
}
```

### Environment Variables

```env
# Model Configuration
GPT_OSS_MODEL_PATH=./gpt-oss-120b/original
GPT_OSS_GPU_LAYERS=50
GPT_OSS_THREADS=8
GPT_OSS_CONTEXT_LENGTH=8192
GPT_OSS_MAX_TOKENS=2048
GPT_OSS_TEMPERATURE=0.1

# Performance Tuning
GPT_OSS_BATCH_SIZE=1
GPT_OSS_TOP_P=0.9
GPT_OSS_TOP_K=40
GPT_OSS_REPETITION_PENALTY=1.1
```

## üîç Troubleshooting

### Common Issues

#### 1. Model Not Found
```
Error: Model path not accessible
```
**Solution**: Run the installation script and verify the model path in `.env`

#### 2. Out of Memory
```
Error: CUDA out of memory
```
**Solution**: Reduce `GPT_OSS_GPU_LAYERS` or use CPU-only mode (`GPT_OSS_GPU_LAYERS=0`)

#### 3. Slow Performance
```
Warning: Generation taking longer than expected
```
**Solutions**:
- Increase `GPT_OSS_GPU_LAYERS` if you have VRAM available
- Reduce `GPT_OSS_MAX_TOKENS` for shorter responses
- Increase `GPT_OSS_THREADS` for CPU processing

#### 4. Python Import Error
```
Error: gpt_oss module not found
```
**Solution**: Install the Python package: `pip install gpt-oss`

### Performance Optimization

#### GPU Optimization
```env
# For RTX 4090 (24GB VRAM)
GPT_OSS_GPU_LAYERS=60
GPT_OSS_BATCH_SIZE=2

# For RTX 3080 (10GB VRAM)
GPT_OSS_GPU_LAYERS=25
GPT_OSS_BATCH_SIZE=1

# For CPU-only systems
GPT_OSS_GPU_LAYERS=0
GPT_OSS_THREADS=16
```

#### Memory Management
- Monitor system RAM usage during operation
- Close other applications to free memory
- Consider using swap space for very large models

## üìä Performance Benchmarks

### Response Times (RTX 4090)
- **Simple Query**: 2-5 seconds
- **Medical Analysis**: 10-30 seconds
- **Complex Diagnosis**: 30-60 seconds

### Token Generation Speed
- **GPU (RTX 4090)**: 15-25 tokens/second
- **GPU (RTX 3080)**: 8-15 tokens/second
- **CPU (16-core)**: 1-3 tokens/second

### Memory Usage
- **Model Loading**: ~120GB RAM
- **Inference**: +8-16GB RAM
- **GPU VRAM**: 10-24GB (depending on layers)

## üîÑ Integration with Synapse AI

### Hybrid Model Strategy

GPT-OSS-120B integrates seamlessly with your existing Synapse AI flows:

1. **Primary Local Processing**: Use GPT-OSS for sensitive data
2. **Cloud Fallback**: Automatic fallback to Gemini models
3. **Performance Comparison**: Built-in benchmarking
4. **Cost Optimization**: Balance between local and cloud processing

### Medical Workflow Integration

```typescript
// Use GPT-OSS for initial analysis
const localDiagnosis = await gptOssDiagnosis(imageData);

// Fallback to cloud models if needed
if (localDiagnosis.confidence < 0.7) {
  const cloudDiagnosis = await aiAssistedDiagnosis(imageData);
  // Compare and combine results
}
```

## üõ°Ô∏è Security & Privacy

### Data Privacy Benefits
- **Local Processing**: No data leaves your system
- **HIPAA Compliance**: Enhanced patient data protection
- **Audit Trail**: Complete control over processing logs
- **No Cloud Dependencies**: Reduced external service risks

### Security Considerations
- Model files are stored locally and should be secured
- Network access not required for inference
- Consider encrypting model files for additional security
- Regular security updates for the Python environment

## üìà Monitoring & Analytics

### Built-in Metrics
- Response time tracking
- Token generation speed
- Memory usage monitoring
- Error rate tracking
- Model performance comparison

### Performance Dashboard
Access real-time metrics through:
- `/api/gpt-oss` endpoint for status
- Built-in performance monitoring
- Integration with Synapse AI analytics

## üîÆ Future Enhancements

### Planned Features
- **Model Fine-tuning**: Custom medical model training
- **Multi-GPU Support**: Distributed processing
- **Model Quantization**: Reduced memory requirements
- **Streaming API**: Real-time response streaming
- **Custom Prompts**: Specialized medical prompt templates

### Roadmap
- Q1 2024: Model fine-tuning capabilities
- Q2 2024: Multi-GPU distributed processing
- Q3 2024: Custom medical model training
- Q4 2024: Advanced optimization features

## üìû Support

### Getting Help
1. Check the troubleshooting section above
2. Run diagnostic tests: `npm run test:gpt-oss`
3. Review system requirements and configuration
4. Check GitHub issues for known problems

### Contributing
- Report bugs and issues
- Suggest performance optimizations
- Contribute medical prompt improvements
- Share benchmark results

---

**GPT-OSS-120B Integration** - Bringing powerful local AI to medical imaging analysis with complete privacy and control.